{"question": "Show me research on rejection sampling finetuning.", "answer": ["Statistical Rejection Sampling Improves Preference Optimization", "Scaling Relationship on Learning Mathematical Reasoning with Large\n  Language Models", "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models", "Self-play with Execution Feedback: Improving Instruction-following   Capabilities of Large Language Models", "Let AI Entertain You: Increasing User Engagement with Generative AI and\n  Rejection Sampling", "Xwin-LM: Strong and Scalable Alignment Practice for LLMs", "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving", "Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations", "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"], "answer_arxiv_id": ["2309.06657", "2308.01825", "2402.10038", "2406.13542", "2312.12457", "2405.20335", "2407.13690", "2310.20246", "2402.18571"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_27"}