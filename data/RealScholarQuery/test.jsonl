{"question": "Give me papers which show that using a smaller dataset in large language model pre-training can result in better models than using bigger datasets.", "answer": ["When Less is More: Investigating Data Pruning for Pretraining LLMs at   Scale", "How to Train Data-Efficient LLMs", "Deduplicating Training Data Makes Language Models Better", "AlpaGasus: Training A Better Alpaca with Fewer Data", "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes", "LESS: Selecting Influential Data for Targeted Instruction Tuning", "Automatic Document Selection for Efficient Encoder Pretraining", "Farewell to aimless large-scale pretraining: Influential subset selection for language model", "Babyllama-2: Ensemble-distilled models consistently outperform teachers with limited data."], "answer_arxiv_id": ["2309.04564", "2402.09668", "2107.06499", "2307.08701", "2305.02301", "2402.04333", "2210.10951", "2305.12816", "2409.17312"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_0"}
{"question": "Give me papers that share some insights about how large language models gain in-context learning capability in the process of pre-training.\n", "answer": ["Pretraining task diversity and the emergence of non-Bayesian in-context   learning for regression", "How Do Transformers Learn In-Context Beyond Simple Functions? A Case   Study on Learning with Representations", "Data Distributional Properties Drive Emergent In-Context Learning in   Transformers", "Transformers Learn Higher-Order Optimization Methods for In-Context   Learning: A Study with Linear Models", "Why Can GPT Learn In-Context? Language Models Implicitly Perform   Gradient Descent as Meta-Optimizers", "In-context Pretraining: Language Modeling Beyond Document Boundaries", "Pre-Training to Learn in Context", "What Do Language Models Learn in Context? The Structured Task Hypothesis", "Transformers learn in-context by gradient descent", "In-context Learning and Induction Heads", "Do pretrained Transformers Learn In-Context by Gradient Descent?", "The mechanistic basis of data dependence and abrupt learning in an   in-context classification task", "Explaining Emergent In-Context Learning as Kernel Regression", "How Do Nonlinear Transformers Learn and Generalize in In-Context\n  Learning?", "Asymptotic theory of in-context learning by linear attention", "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks", "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape", "An Explanation of In-context Learning as Implicit Bayesian Inference", "What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization.", "Language Models \"\"Grok\"\" to Copy", "Transformers as statisticians: Provable in-context learning with in-context algorithm selection", "Understanding in-context learning via supportive pretraining data", "Transformers generalize differently from information stored in context vs in weights.", "Language models are few-shot learners", "Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning", "The transient nature of emergent in-context learning in transformers", "The learnability of in-context learning", "A theory of emergent in-context learning as implicit structure induction", "Parallel Structures in Pre-training Data Yield In-Context Learning"], "answer_arxiv_id": ["2306.15063", "2310.10616", "2205.05055", "2310.17086", "2212.10559", "2310.10638", "2305.09137", "2406.04216", "2212.07677", "2209.11895", "2310.08540", "2312.03002", "2305.12766", "2402.15607", "2405.11751", "2305.17040", "2402.01258", "2111.02080", "2305.19420", "2409.09281", "2306.04637", "2306.15091", "2210.05675", "2005.14165", "2406.14022", "2311.08360", "2303.07895", "2303.07971", "2402.12530"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_1"}
{"question": "List all papers that use autoregressive transformer to generate videos.", "answer": ["VideoPoet: A Large Language Model for Zero-Shot Video Generation", "Patch-based Object-centric Transformers for Efficient Video Generation", "ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models", "Genie: Generative Interactive Environments", "Generative Video Transformer: Can Objects be the Words?", "Latent Video Transformer", "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator", "iVideoGPT: Interactive VideoGPTs are Scalable World Models", "Axial Attention in Multidimensional Transformers", "Godiva: Generating open-domain videos from natural descriptions", "Video Prediction by Efficient Transformers", "Temporally consistent transformers for video generation", "Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image", "VPTR: Efficient Transformers for Video Prediction", "Cogvideo: Large-scale pretraining for text-to-video generation via transformers", "Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis", "Videogpt: Video generation using vq-vae and transformers", "Snap video: Scaled spatiotemporal transformers for text-to-video synthesis", "Pandora: Towards general world model with natural language actions and video states.", "Scaling Autoregressive Video Models", "Emu3: Next token prediction is all you need"], "answer_arxiv_id": ["2312.14125", "2206.04003", "2406.10981", "2402.15391", "2107.09240", "2006.10704", "2209.07143", "2405.15223", "1912.12180", "2104.14806", "2212.06026", "2210.02396", "2203.09457", "2203.15836", "2205.15868", "2207.09814", "2104.10157", "2402.14797", "2406.09455", "1906.02634", "2409.18869"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_2"}
{"question": "I am looking for research papers on the construction of multimodal foundation models that support both visual and audio inputs. These models should be pre-trained on large-scale datasets, including visual, audio, and audio-visual data. Please exclude survey papers.", "answer": ["MERLOT Reserve: Neural Script Knowledge through Vision and Language and   Sound", "InternVideo2: Scaling Video Foundation Models for Multimodal Video   Understanding", "VATT: Transformers for Multimodal Self-Supervised Learning from Raw   Video, Audio and Text", "Fine-grained Audio-Visual Joint Representations for Multimodal Large   Language Models", "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and   Dataset", "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and   Dataset", "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for   Speech Representation Learning", "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model   for Multimodal Processing", "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video   Understanding", "VideoPoet: A Large Language Model for Zero-Shot Video Generation", "LanguageBind: Extending Video-Language Pretraining to N-modality by   Language-based Semantic Alignment", "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models", "Gemini: A Family of Highly Capable Multimodal Models", "Look, Listen and Learn", "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and   Text Integration", "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "Video Understanding as Machine Translation", "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data", "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos", "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation", "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", "Connecting multi-modal contrastive representations", "MIO: A Foundation Model on Multimodal Tokens", "Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action", "Imagebind: One embedding space to bind them all", "Ofasys: A multi-modal multi-task learning system for building generalist models.", "Audioclip: Extending clip to image, text and audio", "Audio-Visual LLM for Video Understanding", "Omnibind: Large-scale omni multimodal representation via binding spaces", "Next-gpt: Any-to-any multimodal llm", "Onellm: One framework to align all modalities with language", "VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "Polyvit: Co-training vision transformers on images, videos and audio", "i-code: An integrative and composable multimodal learning framework", "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation", "One-peace: Exploring one general representation model toward unlimited modalities", "Pandagpt: One model to instruction-follow them all.", "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models", "Freebind: Free lunch in unified multimodal space via knowledge fusion", "Efficient self-supervised learning with contextualized target representations for vision, speech and language", "Extending multi-modal contrastive representations", "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation"], "answer_arxiv_id": ["2201.02639", "2403.15377", "2104.11178", "2310.05863", "2304.08345", "2305.18500", "2211.11275", "2401.12264", "2306.02858", "2312.14125", "2310.01852", "2406.15704", "2312.11805", "1705.08168", "2306.09093", "2408.05211", "2006.07203", "2305.12311", "2006.09199", "2107.00249", "2307.08581", "2305.14381", "2409.17692", "2312.17172", "2305.05665", "2212.04408", "2106.13043", "2312.06720", "2407.11895", "2309.05519", "2312.03700", "2406.07476", "2111.12993", "2205.01818", "2311.18775", "2305.11172", "2305.16355", "2311.13435", "2405.04883", "2212.07525", "2310.08884", "2409.19132"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_3"}
{"question": "Provide me with all papers that discuss reinforcement learning training for Large Language Model agent tasks.", "answer": ["True Knowledge Comes from Practice: Aligning LLMs with Embodied   Environments via Reinforcement Learning", "Teaching Large Language Models to Reason with Reinforcement Learning", "Language Agents with Reinforcement Learning for Strategic Play in the   Werewolf Game", "Enabling Intelligent Interactions between an Agent and an LLM: A   Reinforcement Learning Approach", "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "Reflexion: Language Agents with Verbal Reinforcement Learning", "Guiding Pretraining in Reinforcement Learning with Large Language Models", "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models", "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs", "Collaborating with language models for embodied reasoning", "WebGPT: Browser-assisted question-answering with human feedback", "Grounding Large Language Models in Interactive Environments with Online   Reinforcement Learning", "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study", "LLM Augmented Hierarchical Agents", "Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement", "Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation", "RL-GPT: Integrating Reinforcement Learning and Code-as-policy", "Motif: Intrinsic Motivation from Artificial Intelligence Feedback", "Training Language Models to Self-Correct via Reinforcement Learning", "Math-shepherd: Verify and reinforce llms step-by-step without human annotations", "Generative job recommendations with large language model", "Reinforcement Learning Problem Solving with Large Language Models", "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search", "Enhance reasoning for large language models in the game werewolf", "Reward Design with Language Models", "Adarefiner: Refining decisions of language models with adaptive feedback", "Large Language Models as Generalizable Policies for Embodied Tasks", "Lagr-seq: Language-guided reinforcement learning with sample-efficient querying.", "Eureka: Human-Level Reward Design via Coding Large Language Models", "Openagi: When llm meets domain experts", "Retroformer: Retrospective large language agents with policy gradient optimization", "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning", "Reason for future, act for now: A principled framework for autonomous llm agents with provable sample efficiency", "Towards a unified agent with foundation models", "Selective perception: Optimizing state descriptions with reinforcement learning for language model actors", "Alpacafarm: A simulation framework for methods that learn from human feedback", "Improving alignment of dialogue agents via targeted human judgements", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs.", "Response-act guided reinforced dialogue generation for mental health counseling", "Language models are few-shot butlers", "How Can LLM Guide RL? A Value-Based Approach", "Secrets of RLHF in Large Language Models Part I: PPO", "AGILE: A Novel Framework of LLM Agents"], "answer_arxiv_id": ["2401.14151", "2403.04642", "2310.18940", "2306.03604", "2402.19446", "2303.11366", "2302.06692", "2406.05872", "2404.18978", "2302.00763", "2112.09332", "2302.02662", "2401.06603", "2311.05596", "2402.06700", "2401.00006", "2402.19299", "2310.00166", "2409.12917", "2312.08935", "2307.02157", "2404.18638", "2405.15383", "2402.02330", "2303.00001", "2309.17176", "2310.17722", "2308.13542", "2310.12931", "2304.04370", "2308.02151", "2310.20587", "2309.17382", "2307.09668", "2307.11922", "2305.14387", "2209.14375", "2402.12914", "2305.08844", "2301.12729", "2104.07972", "2402.16181", "2307.04964", "2405.14751"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_4"}
{"question": "Papers that apply RLHF to address the hallucination problem in image and video description.", "answer": ["Aligning Large Multimodal Models with Factually Augmented RLHF", "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning", "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling", " Silkie: Preference distillation for large visual language models", "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback", "Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback", "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback"], "answer_arxiv_id": ["2309.14525", "2402.11411", "2402.06118", "2312.10665", "2311.10081", "2403.06735", "2312.00849"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_5"}
{"question": "Papers that propose methods based on large language models and evaluate their performance through experiments on the HotPotQA dataset.", "answer": ["AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With   Large Language Models", "GenDec: A robust generative Question-decomposition method for Multi-hop   reasoning", "Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open   Domain Multi-Hop Question Answering", "ReAct: Synergizing Reasoning and Acting in Language Models", "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs", "FireAct: Toward Language Agent Fine-tuning", "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition", "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models", "GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?", "Interleaving Retrieval with Chain-of-Thought Reasoning for   Knowledge-Intensive Multi-Step Questions", "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues", "RAFT: Adapting Language Model to Domain Specific RAG", "Recitation-Augmented Language Models", "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach", "Learning to Decompose: Hypothetical Question Decomposition Based on   Comparable Texts", "Language Agent Tree Search Unifies Reasoning Acting and Planning in   Language Models", "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models", "Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process", "Chain-of-Skills: A Configurable Model for Open-domain Question Answering", "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs", "Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought", "Few-shot reranking for multi-hop qa via language model prompting", "Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models", "Answering questions by meta-reasoning over multiple chains of thought"], "answer_arxiv_id": ["2305.15064", "2402.11166", "2403.12393", "2210.03629", "2406.15319", "2310.05915", "2402.11924", "2404.09129", "2407.10245", "2212.10509", "2405.13021", "2403.10131", "2210.01296", "2407.13101", "2210.16865", "2310.04406", "2305.18323", "2402.19350", "2305.03130", "2406.06027", "2404.03414", "2205.12650", "2305.14323", "2304.13007"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_6"}
{"question": "Show me research on the long video description. Here, long videos are defined as those with a duration of at least several minutes.", "answer": ["Video ReCap: Recursive Captioning of Hour-Long Videos", "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding", "Streaming Long Video Understanding with Large Language Models", "Learning To Recognize Procedural Activities with Distant Supervision", "HowToCaption: Prompting LLMs to Transform Video Annotations at Scale", "LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding", "Temporal Alignment Networks for Long-term Video", "Enhancing Long Video Understanding via Hierarchical Event-Based Memory", "DrVideo: Document Retrieval Based Long Video Understanding", "LVBench: An Extreme Long Video Understanding Benchmark", "Long-Form Video-Language Pre-Training with Multimodal Temporal   Contrastive Learning", "World Model on Million-Length Video And Language With Blockwise RingAttention", "Koala: Key frame-conditioned long video-LLM", "Dense-Captioning Events in Videos", "MLLM as Video Narrator: Mitigating Modality Imbalance in Video Moment Retrieval", "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding", "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding", "LongVLM: Efficient Long Video Understanding via Large Language Models", "MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding", "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input", "LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos", "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions", "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs", "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding", "VideoAgent: Long-form Video Understanding with Large Language Model as Agent", "Semi-parametric video-grounded text generation", "Llama-vid: An image is worth 2 tokens in large language models.", "A simple recipe for contrastively pre-training video-first encoders beyond 16 frames.", "Learning Video Representations from Large Language Models", "MM-VID: Advancing Video Understanding with GPT-4V(ision)", "Synopses of movie narratives: a video-language dataset for story understanding", "Mm-narrator: Narrating long-form videos with multimodal in-context learning.", "Videoxum: Cross-modal visual and textural summarization of videos", "Videoagent: A memory-augmented multimodal agent for video understanding.", "Streaming dense video captioning"], "answer_arxiv_id": ["2402.13250", "2406.14515", "2405.16009", "2201.10990", "2310.04900", "2407.15754", "2204.02968", "2409.06299", "2406.12846", "2406.08035", "2210.06031", "2402.08268", "2404.04346", "1705.00754", "2406.17880", "2312.02051", "2404.05726", "2404.03384", "2406.04264", "2408.15542", "2312.05269", "2406.04325", "2402.13546", "2307.16449", "2403.10517", "2301.11507", "2311.17043", "2312.07395", "2212.04501", "2310.19773", "2203.05711", "2311.17435", "2303.12060", "2403.11481", "2404.01297"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_7"}
{"question": "Do you know some papers about using reward shaping methods to train large language model agent.", "answer": ["Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language   Model Critique in Text Generation", "Retroformer: Retrospective Large Language Agents with Policy Gradient   Optimization", "Reward Design with Language Models", "Quark: Controllable Text Generation with Reinforced Unlearning", "Dense Reward for Free in Reinforcement Learning from Human Feedback", "Language Reward Modulation for Pretraining Reinforcement Learning", "Fine-Grained Human Feedback Gives Better Rewards for Language Model   Training", "Self-Rewarding Language Models", "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft", "Token-level Direct Preference Optimization", "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning", "Motif: Intrinsic Motivation from Artificial Intelligence Feedback", "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL", "Simpo: Simple preference optimization with a reference-free reward", "Math-shepherd: Verify and reinforce llms step-by-step without human annotations", "Guiding pretraining in reinforcement learning with large language models"], "answer_arxiv_id": ["2401.07382", "2308.02151", "2303.00001", "2205.13636", "2402.00782", "2308.12270", "2306.01693", "2401.10020", "2312.09238", "2404.11999", "2405.15194", "2310.00166", "2409.12798", "2405.14734", "2312.08935", "2302.06692"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_8"}
{"question": "Give me papers about how to rank search results by the use of LLM.", "answer": ["Instruction Distillation Makes Large Language Models Efficient Zero-shot   Rankers", "Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring   Fine-Grained Relevance Labels", "Large Language Models are Effective Text Rankers with Pairwise Ranking   Prompting", "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking   with Large Language Models", "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large   Language Models", "PaRaDe: Passage Ranking using Demonstrations with Large Language Models", "Is ChatGPT Good at Search? Investigating Large Language Models as   Re-Ranking Agents", "Large Language Models are Zero-Shot Rankers for Recommender Systems", "TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy", "ExaRanker: Explanation-Augmented Neural Ranker", "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs", "Make Large Language Model a Better Ranker", "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "Zero-Shot Listwise Document Reranking with a Large Language Model", "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models", "Large Language Models for Relevance Judgment in Product Search", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "When Search Engine Services meet Large Language Models: Visions and Challenges", "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a   Breeze!", "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on   Open-Source Large Language Models", "MuGI: Enhancing Information Retrieval through Multi-Text Generation Integration with Large Language Models", "Discrete Prompt Optimization via Constrained Generation for Zero-shot   Re-ranker", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain   Question Answering", "Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting   Using Multi-agent LLM", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "Leveraging LLMs for Unsupervised Dense Retriever Ranking", "Unsupervised Contrast-Consistent Ranking with Language Models", "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models", "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models", "Fine-Tuning LLaMA for Multi-Stage Text Retrieval", "Zero-shot Audio Topic Reranking using Large Language Models", "Uncovering ChatGPT's Capabilities in Recommender Systems", "Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism", "Towards More Relevant Product Search Ranking Via Large Language Models: An Empirical Study", "Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction", "Open-source large language models are strong zero-shot query likelihood models for document ranking"], "answer_arxiv_id": ["2311.01555", "2310.14122", "2306.17563", "2310.09497", "2309.15088", "2310.14408", "2304.09542", "2305.08845", "2406.11678", "2301.10521", "2407.02485", "2403.19181", "2406.00231", "2406.13331", "2305.02156", "2404.11791", "2406.18740", "2406.00247", "2404.18424", "2405.20654", "2407.00128", "2312.02724", "2312.02969", "2401.06311", "2305.13729", "2402.17497", "2312.15450", "2406.15657", "2402.04853", "2309.06991", "2403.18093", "2310.07712", "2310.08319", "2309.07606", "2305.02182", "2402.10548", "2409.17460", "2306.01599", "2310.13243"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_9"}
{"question": "Is there any work that analyzes the scaling law of the multi-module models, such as video-text, image-text models?", "answer": ["Scaling Laws for Autoregressive Generative Modeling", "Scaling Laws for Generative Mixed-Modal Language Models", "Reproducible scaling laws for contrastive language-image learning", "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model", "Scaling Up Vision-Language Pre-training for Image Captioning", "Are Bigger Encoders Always Better in Vision Large Models?", "EVA: Exploring the Limits of Masked Visual Representation Learning at   Scale", "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "Scaling Law Hypothesis for Multimodal Model", "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models", "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models", "Multimodal contrastive learning with limoe: the language-image mixture of experts", "Simple open-vocabulary object detection with vision transformers", "Scaling language-image pre-training via masking", "Scaling rectified flow transformers for high-resolution image synthesis"], "answer_arxiv_id": ["2010.14701", "2301.03728", "2212.07143", "2408.11039", "2111.12233", "2408.00620", "2211.07636", "2209.06794", "2409.06754", "2309.09958", "2402.05935", "2206.02770", "2205.06230", "2212.00794", "2403.03206"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_10"}
{"question": "Give me all visual-LLM models that are MoE architecture", "answer": ["Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of   Low-rank Experts", "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts", "EVLM: An Efficient Vision-Language Model for Visual Understanding", "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture   of Experts", "Mixture of Cluster-conditional LoRA Experts for Vision-language   Instruction Tuning", "MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models", "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts", "Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model", "VLMo: Unified Vision-Language Pre-Training with   Mixture-of-Modality-Experts", "EVE: Efficient Vision-Language Pre-training with Masked Prediction and   Modality-Aware MoE", "Boosting Continual Learning of Vision-Language Models via   Mixture-of-Experts Adapters", "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts   in Instruction Finetuning MLLMs", "Mini-Gemini: Mining the Potential of Multi-modality Vision Language   Models", "MoExtend: Tuning New Experts for Modality and Task Extension", "Scaling Vision-Language Models with Sparse Mixture of Experts", "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving", "LLMBind: A Unified Modality-Task Integration Framework", "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"], "answer_arxiv_id": ["2312.00968", "2401.15947", "2405.05949", "2407.14177", "2206.02770", "2312.12379", "2407.12709", "2405.11273", "2406.19905", "2111.02358", "2308.11971", "2403.11549", "2407.21770", "2401.16160", "2403.18814", "2408.03511", "2303.07226", "2409.07267", "2402.14891", "2403.09611"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_11"}
{"question": "What papers discuss the use of transformer architecture in 3d video generation", "answer": ["VideoGPT: Video Generation using VQ-VAE and Transformers", "VDT: General-purpose Video Diffusion Transformers via Mask Modeling", "TEACH: Temporal Action Composition for 3D Humans", "Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene   Video from A Single Image", "MAGVIT: Masked Generative Video Transformer", "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic   Memory", "Tora: Trajectory-oriented Diffusion Transformer for Video Generation", "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer", "Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape", "Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text", "Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers", "Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer", "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++", "EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture", "N\\\"UWA: Visual Synthesis Pre-training for Neural visUal World creAtion", "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control", "Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer"], "answer_arxiv_id": ["2104.10157", "2305.13311", "2209.04066", "2203.09457", "2212.05199", "2203.13055", "2407.21705", "2103.10206", "2310.20240", "2406.17601", "2405.05945", "2405.17405", "2101.08779", "2405.18991", "2111.12417", "2407.12781", "2204.03638"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_12"}
{"question": "Provide papers demonstrating that the self-correction of LLMs does not enhance their performance.", "answer": ["Can Large Language Models Really Improve by Self-critiquing Their Own   Plans?", "Are You Sure? Challenging LLMs Leads to Performance Drops in The   FlipFlop Experiment", "A Closer Look at the Self-Verification Abilities of Large Language   Models in Logical Reasoning", "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses", "Large Language Models Cannot Self-Correct Reasoning Yet", "Is Self-Repair a Silver Bullet for Code Generation?", "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks", "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of   Their Incorrect Generations?", "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs", "On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept", "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement", "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems"], "answer_arxiv_id": ["2310.08118", "2311.08596", "2311.07954", "2404.04298", "2310.01798", "2306.09896", "2402.08115", "2402.19475", "2406.01297", "2406.02378", "2402.11436", "2310.12397"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_13"}
{"question": "Find papers that use LLMs or LLM-based agents to automatically write surveys or summaries for multiple scholarly documents.", "answer": ["AutoSurvey: Large Language Models Can Automatically Write Surveys", "Instruct Large Language Models to Generate Scientific Literature Survey Step by Step", "LitLLM: A Toolkit for Scientific Literature Review", "vitaLITy 2: Reviewing Academic Literature Using Large Language Models", "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3   (with Varying Success)", "ChatCite: LLM Agent with Human Workflow Guidance for Comparative   Literature Summary", "System for systematic literature review using multiple AI agents:   Concept and an empirical evaluation", "Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications", "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning"], "answer_arxiv_id": ["2406.10252", "2408.07884", "2402.01788", "2408.13450", "2305.06299", "2403.02574", "2403.08399", "2409.18454", "2404.08680"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_14"}
{"question": "Provide papers claiming that reinforcement learning can negatively impact the performance of supervised fine-tuned LLMs.", "answer": ["Understanding the Effects of RLHF on LLM Generalisation and Diversity", "Fundamental Limitations of Alignment in Large Language Models", "Reward Collapse in Aligning Large Language Models", "Discovering Language Model Behaviors with Model-Written Evaluations", "Vanishing gradients in reinforcement finetuning of language models"], "answer_arxiv_id": ["2310.06452", "2304.11082", "2305.17608", "2212.09251", "2310.20703"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_15"}
{"question": "Find papers on trigger-free document-level event extraction methods that do not use human-annotated triggers.", "answer": ["Trigger-free Event Detection via Derangement Reading Comprehension", "COFFEE: A Contrastive Oracle-Free Framework for Event Extraction", "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial   Event Extraction", "Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph", "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization", "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction", "Document-Level Event Extraction via Human-Like Reading Process", "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker"], "answer_arxiv_id": ["2208.09659", "2303.14452", "1904.07535", "2112.06013", "2305.18926", "2206.03377", "2202.03092", "2105.14924"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_16"}
{"question": "Provide papers explaining why the in-context learning performance of LLMs cannot surpass that of supervised fine-tuned small language models in information extraction tasks, such as NER, RE, and EE.", "answer": ["Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again", "When does In-context Learning Fall Short and Why? A Study on   Specification-Heavy Tasks", "Intent Detection and Entity Extraction from BioMedical Literature", "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks", "Exploring the Feasibility of ChatGPT for Event Extraction", "Guideline Learning for In-context Information Extraction", "Pushing the Limits of ChatGPT on NLP Tasks", "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!", "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling"], "answer_arxiv_id": ["2203.08410", "2311.08993", "2404.03598", "2404.00457", "2303.03836", "2310.05066", "2306.09719", "2303.08559", "2401.14556"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_17"}
{"question": "Can LLMs detect LLM-generated text in a zero-shot manner? Do they perform better than supervised fine-tuned small classification models? Provide related papers.", "answer": ["MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark", "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore", "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness", "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions.", "Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation"], "answer_arxiv_id": ["2310.13606", "2405.04286", "2409.16914", "2310.14479", "2310.15515"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_18"}
{"question": "Provide papers on methods that protect the generation quality of LLMs under vocabulary watermarking settings.", "answer": ["A Resilient and Accessible Distribution-Preserving Watermark for Large\n  Language Models", "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative   Large Language Models", "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language   Models", "Token-Specific Watermarking with Enhanced Detectability and Semantic   Coherence for Large Language Models", "CATER: Intellectual Property Protection on Text Generation APIs via   Conditional Watermarks", "Protecting Intellectual Property of Language Generation APIs with   Lexical Watermark", "WatME: Towards Lossless Watermarking Through Lexical Redundancy", "Duwak: Dual Watermarks in Large Language Models", "Provable Robust Watermarking for AI-Generated Text", "Adaptive Text Watermark for Large Language Models", "A Watermark for Large Language Models", "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality", "Optimizing watermarks for large language models", "PostMark: A Robust Blackbox Watermark for Large Language Models", "Necessary and Sufficient Watermark for Large Language Models", "Undetectable Watermarks for Language Models", "Protecting Language Generation Models via Invisible Watermarking", "Unbiased Watermark for Large Language Models", "Embarrassingly Simple Text Watermarks", "A Watermark for Low-entropy and Unbiased Generation in Large Language Models", "Mark My Words: Analyzing and Evaluating Language Model Watermarks", "Topic-Based Watermarks for LLM-Generated Text", "Watermarking Language Models with Error Correcting Codes", "PersonaMark: Personalized LLM watermarking for model protection and user attribution", "Adversarial Watermarking Transformer: Towards Tracing Text Provenance   with Data Hiding", "ModelShield: Adaptive and Robust Watermark against Model Extraction Attack", "Improving the Generation Quality of Watermarked Large Language Models   via Word Importance Scoring", "WaterMax: breaking the LLM watermark detectability-robustness-quality   trade-off", "Watermarking Conditional Text Generation for AI Detection: Unveiling   Challenges and a Semantic-Aware Watermark Remedy", "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs", "Watermarking Text Generated by Black-Box Language Models", "A Semantic Invariant Robust Watermark for Large Language Models", "Provably Robust Multi-bit Watermarking for AI-generated Text via Error   Correction Code", "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "Cross-Attention Watermarking of Large Language Models", "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "Who wrote this code? watermarking for code generation."], "answer_arxiv_id": ["2310.07710", "2310.12362", "2403.19548", "2402.18059", "2209.08773", "2112.02701", "2311.09832", "2403.13000", "2306.17439", "2401.13927", "2301.10226", "2407.13803", "2312.17295", "2406.14517", "2310.00833", "2306.09194", "2302.03162", "2310.10669", "2310.08920", "2405.14604", "2312.00273", "2404.02138", "2406.10281", "2409.09739", "2009.03015", "2405.02365", "2311.09668", "2403.04808", "2307.13808", "2307.15992", "2305.08883", "2310.06356", "2401.16820", "2308.00221", "2401.06829", "2310.03991", "2305.15060"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_19"}
{"question": "Find papers supporting the claim that knowledgeable LLMs have sufficient inductive capacity to analyze the relationships between multiple papers and systematically write a survey on them.", "answer": ["System for systematic literature review using multiple AI agents:   Concept and an empirical evaluation", "Explaining Relationships Among Research Papers", "Instruct Large Language Models to Generate Scientific Literature Survey Step by Step", "vitaLITy 2: Reviewing Academic Literature Using Large Language Models"], "answer_arxiv_id": ["2403.08399", "2402.13426", "2408.07884", "2408.13450"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_20"}
{"question": "Search for papers related to large language models that demonstrate how the same prompt with different responses can improve the performance of the SFT model.", "answer": ["Curry-DPO: Enhancing Alignment using Curriculum Learning &amp;amp; Ranked Preferences", "Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts"], "answer_arxiv_id": ["2403.07230", "2402.10958"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_21"}
{"question": "Papers on solving common sense problems in machine translation.", "answer": ["Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion\n  Model with Large Language Models for Machine Translation", "Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate", "Few-shot learning with multilingual language models."], "answer_arxiv_id": ["2402.10699", "2305.19118", "2112.10668"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_22"}
{"question": "Show me papers utilizing reinforcement learning to optimize diffusion models for video generation.", "answer": ["Video Diffusion Alignment via Reward Gradients", "InstructVideo: Instructing Video Diffusion Models with Human Feedback"], "answer_arxiv_id": ["2407.08737", "2312.12490"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_23"}
{"question": "Show me all research papers on machine translation agents.", "answer": ["LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text   Translation", "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts\n", "Towards Achieving Human Parity on End-to-end Simultaneous Speech   Translation via LLM Agent", "SiLLM: Large Language Models for Simultaneous Machine Translation", "Dual Learning for Machine Translation", "Learning to Translate in Real-time with Neural Machine Translation", "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large   Language Models", "A Reinforcement Learning Approach to Interactive-Predictive Neural\n  Machine Translation", "Parrot: Translating during chat using large language models tuned with human translation and feedback", "Zero-resource neural machine translation with multi-agent communication game", "Incremental decoding and training methods for simultaneous translation in neural machine translation"], "answer_arxiv_id": ["2407.12126", "2405.11804", "2407.21646", "2402.13036", "1611.00179", "1610.00388", "2406.06910", "1805.01553", "2304.02426", "1802.03116", "1806.03661"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_24"}
{"question": "Video aesthetics score, using multimodal large models.", "answer": ["Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined\n  Levels"], "answer_arxiv_id": ["2312.17090"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_25"}
{"question": "Scaling Laws for Fine-Grained Mixture of Experts.", "answer": ["Scaling Laws for Fine-Grained Mixture of Experts", "Mixture of A Million Experts"], "answer_arxiv_id": ["2402.07871", "2407.04153"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_26"}
{"question": "Show me research on rejection sampling finetuning.", "answer": ["Statistical Rejection Sampling Improves Preference Optimization", "Scaling Relationship on Learning Mathematical Reasoning with Large\n  Language Models", "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models", "Self-play with Execution Feedback: Improving Instruction-following   Capabilities of Large Language Models", "Let AI Entertain You: Increasing User Engagement with Generative AI and\n  Rejection Sampling", "Xwin-LM: Strong and Scalable Alignment Practice for LLMs", "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving", "Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations", "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"], "answer_arxiv_id": ["2309.06657", "2308.01825", "2402.10038", "2406.13542", "2312.12457", "2405.20335", "2407.13690", "2310.20246", "2402.18571"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_27"}
{"question": "Show me code evaluation datasets with a mid-level hardness. It show be harder than HumanEval and MBPP, but easier than code_contests.", "answer": ["PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM", "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution", "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and   Natural User Prompts"], "answer_arxiv_id": ["2401.03855", "2310.06770", "2401.03065", "2405.04520"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_28"}
{"question": "Research on teaching llms to do math prove and solve IMO level math problems.", "answer": ["HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and   Dynamic Workflows", "Large Language Models for Mathematical Reasoning: Progresses and\n  Challenges", "Proving Olympiad Algebraic Inequalities without Human Demonstrations", "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale   Synthetic Data", "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo   Tree Self-refine with LLaMa-3 8B", "Lyra: Orchestrating Dual Correction in Automated Theorem Proving", "InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced   Mathematical Reasoning", "Lean-STaR: Learning to Interleave Thinking and Proving"], "answer_arxiv_id": ["2409.17433", "2402.00157", "2406.14219", "2405.14333", "2406.07394", "2309.15806", "2409.12568", "2407.10040"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_29"}
{"question": "I would like to find some research papers about test time training topic, in LLM research area.", "answer": ["Test-Time Training on Nearest Neighbors for Large Language Models", "Test-Time Training on Graphs with Large Language Models (LLMs)", "Efficient Test-Time Adaptation of Vision-Language Models", "On the test-time zero-shot generalization of vision-language models: Do   we really need prompt learning?", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models   towards Medical Reasoning", "Self-Refine: Iterative Refinement with Self-Feedback"], "answer_arxiv_id": ["2305.18466", "2404.13571", "2403.18293", "2405.02266", "2405.03000", "2303.17651"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_30"}
{"question": "DPO training for large-scale vision-language models.", "answer": ["mDPO: Conditional Preference Optimization for Multimodal Large Language   Models", "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware\n  Direct Preference Optimization", "Mitigating Multilingual Hallucination in Large Vision-Language Models", "Detecting and Preventing Hallucinations in Large Vision Language Models", "Silkie: Preference Distillation for Large Visual Language Models", "Detecting and Mitigating Hallucination in Large Vision Language Models   via Fine-Grained AI Feedback", "Multi-modal preference alignment remedies regression of visual\n  instruction tuning on language model", "Aligning Modalities in Vision Large Language Models via Preference\n  Fine-tuning", "Enhancing Large Vision Language Models with Self-Training on Image   Comprehension", "Direct Preference Optimization of Video Large Multimodal Models from   Language Model Reward", "STLLaVA-Med: Self-Training Large Language and Vision Assistant for   Medical Question-Answering", "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing   Hallucinations in LVLMs", "Diffusion Model Alignment Using Direct Preference Optimization", "Multi-Modal Hallucination Control by Visual Information Grounding", "Automated Multi-level Preference for MLLMs"], "answer_arxiv_id": ["2406.11839", "2311.16839", "2408.00550", "2308.06394", "2312.10665", "2404.14233", "2402.10884", "2402.11411", "2405.19716", "2404.01258", "2406.19973", "2408.10433", "2311.12908", "2403.14003", "2405.11165"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_31"}
{"question": "Show me cutting edge research works on neural network based quantum Monte Carlo.", "answer": ["Neural-network quantum state study of the long-range antiferromagnetic\n  Ising chain", "Universal Performance Gap of Neural Quantum States Applied to the Hofstadter-Bose-Hubbard Model", "Variational Quantum Monte Carlo Method with a Neural-Network Ansatz for\n  Open Quantum Systems", "Discovering Quantum Phase Transitions with Fermionic Neural Networks", "Ab-initio quantum chemistry with neural-network wavefunctions", "Second-order optimisation strategies for neural network quantum states", "NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems", "Deep learning quantum Monte Carlo for solids", "Neural network quantum state with proximal optimization: a ground-state\n  searching scheme based on variational Monte Carlo", "Variational Monte Carlo with Neural Network Quantum States for Yang-Mills Matrix Model", "Natural Quantum Monte Carlo Computation of Excited States", "Neural Quantum States in Variational Monte Carlo Method: A Brief Summary", "Forward Laplacian: A New Computational Framework for Neural\n  Network-based Variational Monte Carlo", "Solving the nuclear pairing model with neural network quantum states", "Highly Accurate Real-space Electron Densities with Neural Networks", "Penalty and auxiliary wave function methods for electronic Excitation in neural network variational Monte Carlo"], "answer_arxiv_id": ["2308.09709", "2405.01981", "1902.09483", "2202.05183", "2208.12590", "2401.17550", "2112.10526", "2407.00707", "2210.16493", "2409.00398", "2308.16848", "2406.01017", "2307.08214", "2211.04614", "2409.01306", "2311.17595"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_32"}
{"question": "Show me some popular papers on generating textual adversarial examples for machine translation.", "answer": ["A Classification-Guided Approach for Adversarial Attacks against Neural\n  Machine Translation", "A Reinforced Generation of Adversarial Examples for Neural Machine\n  Translation", "On Adversarial Examples for Character-Level Neural Machine Translation", "A Targeted Attack on Black-Box Neural Machine Translation with Parallel\n  Data Poisoning", "Sentiment Perception Adversarial Attacks on Neural Machine Translation\n  Systems", "PAEG: Phrase-level Adversarial Example Generation for Neural Machine\n  Translation", "Targeted Adversarial Attacks against Neural Machine Translation", "Generating Authentic Adversarial Examples beyond Meaning-preserving with\n  Doubly Round-trip Translation", "Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with\n  Adversarial Examples", "TransFool: An Adversarial Attack against Neural Machine Translation\n  Models", "Rethinking Targeted Adversarial Attacks For Neural Machine Translation", "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text   against Neural Machine Translation", "Robust Neural Machine Translation with Doubly Adversarial Inputs"], "answer_arxiv_id": ["2308.15246", "1911.03677", "1806.09030", "2011.00675", "2305.01437", "2201.02009", "2303.01068", "2204.08689", "1803.01128", "2302.00944", "2407.05319", "2409.05021", "1906.02443"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_33"}
{"question": "Show me research on 3d scene understanding leveraging progress on 3D AIGC foundation models.", "answer": ["3D-VLA: A 3D Vision-Language-Action Generative World Model", "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene   Understanding", "ConceptFusion: Open-set Multimodal 3D Mapping", "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding", "3D-VirtFusion: Synthetic 3D Data Augmentation through Generative Diffusion Models and Controllable Editing", "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding", "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models"], "answer_arxiv_id": ["2403.09631", "2409.03757", "2302.07241", "2401.09340", "2408.13788", "2401.01970", "2305.08776"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_34"}
{"question": "Give me papers about LLM quantized pretraining.", "answer": ["The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits", "FP8-LM: Training FP8 Large Language Models", "LoQT: Low-Rank Adapters for Quantized Pretraining", "Exploring Quantization for Efficient Pre-Training of Transformer   Language Models", "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients", "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization", "Training and inference of large language models using 8-bit floating point"], "answer_arxiv_id": ["2402.17764", "2310.18313", "2405.16528", "2407.11722", "2407.08296", "2403.12422", "2309.17224"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_35"}
{"question": "Show me research on identity preservation video generation.", "answer": ["HeadGAN: One-shot Neural Head Synthesis and Editing", "MagicPose: Realistic Human Poses and Facial Expressions Retargeting with\n  Identity-aware Diffusion", "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation", "Everybody's Talkin': Let Me Talk as You Want", "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "A Latent Transformer for Disentangled Face Editing in Images and Videos", "CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects", "Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control", "Anchored Diffusion for Video Face Reenactment", "VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via   Diffusion Transformers", "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions", "One-Shot Identity-Preserving Portrait Reenactment", "Towards Realistic Visual Dubbing with Heterogeneous Sources", "An Identity-Preserved Framework for Human Motion Transfer", "Infinite-ID: Identity-preserved Personalization via ID-semantics\n  Decoupling Paradigm", "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video   Generation", "Neural Style-Preserving Visual Dubbing", "Identity-Preserving Talking Face Generation with Landmark and Appearance\n  Priors", "ID-Animator: Zero-Shot Identity-Preserving Human Video Generation", "X2Face: A network for controlling face generation by using images,\n  audio, and pose codes", "Deep Video Portraits", "MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen\n  Targets", "Facial Expression Video Generation Based-On Spatio-temporal\n  Convolutional GAN: FEV-GAN", "MIMAFace: Face Animation via Motion-Identity Modulated Appearance   Feature Learning", "DreaMoving: A Human Video Generation Framework based on Diffusion Models", "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video\n  Generators", "Magic-Me: Identity-Specific Video Customized Diffusion", "Audio-driven High-resolution Seamless Talking Head Video Editing via   StyleGAN", "Automatic Face Reenactment", "S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for Face Video Editing", "VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model", "MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model", "One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field"], "answer_arxiv_id": ["2012.08261", "2311.12052", "2301.03396", "2001.05201", "2403.08764", "2106.11895", "2401.09962", "2208.02210", "2407.15153", "2405.18326", "2402.17485", "2004.12452", "2201.06260", "2204.06862", "2403.11781", "2405.01434", "1909.02518", "2305.08293", "2404.15275", "1807.10550", "1805.11714", "1911.08139", "2210.11182", "2409.15179", "2312.05107", "2303.13439", "2402.09368", "2407.05577", "1602.02651", "2404.08111", "2311.17338", "2311.16498", "2304.05097"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_36"}
{"question": "Give me some papers showing that LLM agents can do schedule planning.", "answer": ["RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks", "Large Language Models for Power Scheduling: A User-Centric Approach", "Smart Language Agents in Real-World Planning", "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility", "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take   TravelPlanner as an Example", "RePrompt: Planning by Automatic Prompt Engineering for Large Language   Models Agents", "AIOS: LLM Agent Operating System", "Generative Agents: Interactive Simulacra of Human Behavior", "LLMs can Schedule", "Large Language Models as Commonsense Knowledge for Large-Scale Task\n  Planning"], "answer_arxiv_id": ["2311.15649", "2407.00476", "2407.19667", "2407.08550", "2408.06318", "2406.11132", "2403.16971", "2304.03442", "2408.06993", "2305.14078"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_37"}
{"question": "Show me research on image encoding distributions.", "answer": ["BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling", "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG\n  Encoder-Decoder", "Wasserstein Auto-Encoders", "Learning to Improve Image Compression without Changing the Standard\n  Decoder", "End-to-end optimized image compression with competition of prior\n  distributions", "Latent Space Imaging", "Learned Image Compression with Gaussian-Laplacian-Logistic Mixture Model\n  and Concatenated Residual Modules", "PixelVAE: A Latent Variable Model for Natural Images", "Learned Compression of Encoding Distributions", "NICE: Non-linear Independent Components Estimation", "Second Sight: Using brain-optimized encoding models to align image\n  distributions with human brain activity", "Deep Generative Models for Distribution-Preserving Lossy Compression", "CUPID: Contextual Understanding of Prompt-conditioned Image   Distributions", "Distribution prediction for image compression: An experimental\n  re-compressor for JPEG images", "Bridging Distribution Learning and Image Clustering in High-dimensional\n  Space", "Compressing Images by Encoding Their Latent Representations with\n  Relative Entropy Coding", "Learned Compression for Images and Point Clouds"], "answer_arxiv_id": ["1902.02102", "2201.11795", "1711.01558", "2009.12927", "2111.09172", "2407.07052", "2107.06463", "1611.05013", "2406.13059", "1410.8516", "2306.00927", "1805.11057", "2406.07699", "2310.10517", "2308.15667", "2010.01185", "2409.08376"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_38"}
{"question": "Help me search for the work related to the synthetic data of large language models. I want to know how to automatically generate large-scale, high-quality, diverse, difficult, and valuable long thought data for learning.", "answer": ["MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale   Synthetic Data"], "answer_arxiv_id": ["2402.08957", "2405.14333"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_39"}
{"question": "Could you list research that demonstrates the advantages of Quantization-Aware Training (QAT), which can enable the model to learn better representations for low-bit weights?.", "answer": ["Quantizing deep convolutional networks for efficient inference: A\n  whitepaper", "Low-Rank Quantization-Aware Training for LLMs", "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for\n  the Acceleration of Lightweight LLMs on the Edge", "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models", "EfficientQAT: Efficient Quantization-Aware Training for Large Language   Models"], "answer_arxiv_id": ["1806.08342", "2406.06385", "2402.10787", "2305.17888", "2407.11062"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_40"}
{"question": "Using synthesis data for scaling up sft data.", "answer": ["Common 7B Language Models Already Possess Strong Math Capabilities", "API-guided Dataset Synthesis to Finetune Large Code Models", "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs   with Nothing", "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On", "FullAnno: A Data Engine for Enhancing Image Comprehension of MLLMs"], "answer_arxiv_id": ["2403.04706", "2408.08343", "2406.08464", "2407.08348", "2409.13540"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_41"}
{"question": "Show me research on how to select frames when doing video understanding.", "answer": ["Frame attention networks for facial expression recognition in videos", "Multi-Agent Reinforcement Learning Based Frame Sampling for Effective\n  Untrimmed Video Recognition", "BubbleNets: Learning to Select the Guidance Frame in Video Object\n  Segmentation by Deep Sorting Frames", "KeyVideoLLM: Towards Large-scale Video Keyframe Selection", "Unsupervised video summarization framework using keyframe extraction and\n  video skimming", "Key Frame Extraction with Attention Based Deep Neural Networks", "AdaFrame: Adaptive Frame Selection for Fast Video Recognition", "Koala: Key frame-conditioned long video-LLM", "End-to-End Video Question Answering with Frame Scoring Mechanisms and   Adaptive Sampling", "Online Learnable Keyframe Extraction in Videos and its Application with\n  Semantic Word Vector in Action Recognition"], "answer_arxiv_id": ["1907.00193", "1907.13369", "1903.11779", "2407.03104", "1910.04792", "2306.13176", "1811.12432", "2404.04346", "2407.15047", "2009.12434"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_42"}
{"question": "AI for Science papers, especially protein design and DPO of antibody design.", "answer": ["Generative AI for Controllable Protein Sequence Design: A Survey", "Graph Denoising Diffusion for Inverse Protein Folding", "Protein Design with Guided Discrete Diffusion", "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model\n  for Protein Design", "Diffusion Language Models Are Versatile Protein Learners", "Protein structure generation via folding diffusion", "A Text-guided Protein Design Framework", "Protein Structure and Sequence Generation with Equivariant Denoising\n  Diffusion Probabilistic Models", "Protein sequence design with deep generative models", "How to Hallucinate Functional Proteins", "PiFold: Toward effective and efficient protein inverse folding", "ProGen: Language Modeling for Protein Generation", "Protein Design by Integrating Machine Learning with Quantum Annealing   and Quantum-inspired Optimization", "Decomposed Direct Preference Optimization for Structure-Based Drug   Design", "A framework for conditional diffusion modelling with applications in motif scaffolding for protein design", "Fast protein backbone generation with SE(3) flow matching", "SE(3) diffusion model with application to protein backbone generation", "AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB", "Structure-informed Language Models Are Protein Designers", "Preference optimization of protein language models as a multi-objective\n  binder design paradigm", "Deep Generative Modeling for Protein Design", "Antigen-Specific Antibody Design via Direct Energy-based Preference\n  Optimization", "Protein Conformation Generation via Force-Guided SE(3) Diffusion Models", "Generative De Novo Protein Design with Global Context", "Leveraging Deep Generative Model For Computational Protein Design And Optimization", "PDB-Struct: A Comprehensive Benchmark for Structure-based Protein Design", "Controllable Protein Design with Language Models", "Generative artificial intelligence for de novo protein design"], "answer_arxiv_id": ["2402.10516", "2306.16819", "2305.20009", "2106.13058", "2402.18567", "2209.15611", "2302.04611", "2205.15019", "2104.04457", "1903.00458", "2209.12643", "2004.03497", "2407.07177", "2407.13981", "2312.09236", "2310.05297", "2302.02277", "2202.01079", "2302.01649", "2403.04187", "2109.13754", "2403.16576", "2403.14088", "2204.10673", "2408.17241", "2312.00080", "2201.07338", "2310.09685"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_43"}
{"question": "What are the researches that have explored the application of Crypto-based Private Learning in privacy-preserving machine learning?.", "answer": ["CryptoDL: Deep Neural Networks over Encrypted Data", "Privacy-Preserving Machine Learning with Fully Homomorphic Encryption\n  for Deep Neural Network", "Faster CryptoNets: Leveraging Sparsity for Real-World Encrypted\n  Inference", "Neural Network Training With Homomorphic Encryption", "SecureBoost: A Lossless Federated Learning Framework", "Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted\n  Data", "Towards the AlexNet Moment for Homomorphic Encryption: HCNN, theFirst\n  Homomorphic CNN on Encrypted Data with GPUs", "POSEIDON: Privacy-Preserving Federated Neural Network Learning", "SoK: Privacy Preserving Machine Learning using Functional Encryption:\n  Opportunities and Challenges", "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption", "TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service", "CryptoNN: Training Neural Networks over Encrypted Data", "Blind Faith: Privacy-Preserving Machine Learning using Function\n  Approximation", "Privacy-Preserving Logistic Regression Training on Large Datasets", "CryptoGCN: Fast and Scalable Homomorphically Encrypted Graph\n  Convolutional Network Inference", "Decentralised, Collaborative, and Privacy-preserving Machine Learning\n  for Multi-Hospital Data", "Privacy-Preserving Machine Learning: Methods, Challenges and Directions", "Homomorphic Encryption and Federated Learning based Privacy-Preserving\n  CNN Training: COVID-19 Detection Use-Case", "A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving   Machine Learning Through Hybrid Homomorphic Encryption", "Crypto-Nets: Neural Networks over Encrypted Data", "SHE: A Fast and Accurate Deep Neural Network for Encrypted Data", "Private and Reliable Neural Network Inference", "Learning in the Dark: Privacy-Preserving Machine Learning using Function\n  Approximation", "Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure\n  Lookup Table Computation", "Partially Encrypted Machine Learning using Functional Encryption"], "answer_arxiv_id": ["1711.05189", "2106.07229", "1811.09953", "2012.13552", "1901.08755", "1911.07101", "1811.00778", "2009.00349", "2204.05136", "2409.07751", "1806.03461", "1904.07303", "2107.14338", "2406.13221", "2209.11904", "2402.00205", "2108.04417", "2204.07752", "2409.06422", "1412.6181", "1906.00148", "2210.15614", "2309.08190", "2403.17296", "1905.10214"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_44"}
{"question": "All papers about controllability of video generation.", "answer": ["Motion-Conditioned Diffusion Model for Controllable Video Synthesis", "Panacea+: Panoramic and Controllable Video Generation for Autonomous   Driving", "Cinemo: Consistent and Controllable Image Animation with Motion   Diffusion Models", "Magic-Me: Identity-Specific Video Customized Diffusion", "AnimateLCM: Accelerating the Animation of Personalized Diffusion Models\n  and Adapters with Decoupled Consistency Learning", "DrivingDiffusion: Layout-Guided multi-view driving scene video\n  generation with latent diffusion model", "TrailBlazer: Trajectory Control for Diffusion-Based Video Generation", "MoCoGAN: Decomposing Motion and Content for Video Generation", "Genie: Generative Interactive Environments", "Direct-a-Video: Customized Video Generation with User-Directed Camera\n  Movement and Object Motion", "Click to Move: Controlling Video Generation with Sparse Motion", "LaMD: Latent Motion Diffusion for Video Generation", "Structure and Content-Guided Video Synthesis with Diffusion Models", "Collaborative Video Diffusion: Consistent Multi-video Generation with   Camera Control", "Control-A-Video: Controllable Text-to-Video Generation with Diffusion\n  Models", "Text-Animator: Controllable Visual Text Video Generation", "DiVE: DiT-based Video Generation with Enhanced Control", "Understanding Object Dynamics for Interactive Image-to-Video Synthesis", "ControlVideo: Training-free Controllable Text-to-Video Generation", "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for\n  Character Animation", "DriveScape: Towards High-Resolution Controllable Multi-View Driving   Video Generation", "CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation", "PEEKABOO: Interactive Video Generation via Masked-Diffusion", "CameraCtrl: Enabling Camera Control for Text-to-Video Generation", "Video Generation Beyond a Single Clip", "VideoComposer: Compositional Video Synthesis with Motion Controllability", "DragAnything: Motion Control for Anything using Entity Representation", "RefDrop: Controllable Consistency in Image or Video Generation via   Reference Feature Guidance", "Make-Your-Video: Customized Video Generation Using Textual and\n  Structural Guidance", "DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention\n  and Text Guidance", "DragNUWA: Fine-grained Control in Video Generation by Integrating Text,\n  Image, and Trajectory", "MagicDrive: Street View Generation with Diverse 3D Geometry Control", "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models", "Fine-grained Controllable Video Generation via Object Appearance and\n  Context", "Panacea: Panoramic and Controllable Video Generation for Autonomous\n  Driving", "Ctrl-V: Higher Fidelity Video Generation with Bounding-Box Controlled   Object Motion", "LLM-grounded Video Diffusion Models", "Boximator: Generating Rich and Controllable Motions for Video Synthesis", "TrackGo: A Flexible and Efficient Method for Controllable Video   Generation", "Make It Move: Controllable Image-to-Video Generation with Text\n  Descriptions", "MotionCtrl: A Unified and Flexible Motion Controller for Video\n  Generation", "AMG: Avatar Motion Guided Video Generation", "Imagen Video: High Definition Video Generation with Diffusion Models", "VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by\n  Using Diffusion Model with ControlNet", "ControlNeXt: Powerful and Efficient Control for Image and Video   Generation", "MyGo: Consistent and Controllable Multi-View Driving Video Generation   with Camera Control", "Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free\n  Videos", "Moonshot: Towards Controllable Video Generation and Editing with\n  Multimodal Conditions", "Training-free Camera Control for Video Generation", "MotionDirector: Motion Customization of Text-to-Video Diffusion Models", "Stochastic Image-to-Video Synthesis using cINNs", "SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models", "MotionClone: Training-Free Motion Cloning for Controllable Video Generation", "MagicStick: Controllable Video Editing via Control Handle Transformations", "VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models", "DreamVideo: Composing Your Dream Videos with Customized Subject and Motion", "Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling", "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model"], "answer_arxiv_id": ["2304.14404", "2408.07605", "2407.15642", "2402.09368", "2402.00769", "2310.07771", "2401.00896", "1707.04993", "2402.15391", "2402.03162", "2108.08815", "2304.11603", "2302.03011", "2405.17414", "2305.13840", "2406.17777", "2409.01595", "2106.11303", "2305.13077", "2311.17117", "2409.05463", "2406.02509", "2312.07509", "2404.02101", "2304.07483", "2306.02018", "2403.07420", "2405.17661", "2306.00943", "2312.03018", "2308.08089", "2310.02601", "2406.16863", "2312.02919", "2311.16813", "2406.05630", "2309.17444", "2402.01566", "2408.11475", "2112.02815", "2312.03641", "2409.01502", "2210.02303", "2307.14073", "2408.06070", "2409.06189", "2304.01186", "2401.01827", "2406.10126", "2310.08465", "2105.04551", "2311.16933", "2406.05338", "2312.03047", "2312.00845", "2312.04433", "2401.15977", "2405.20222"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_45"}
{"question": "Show me research on robot decision making and task planning, especially relevant datasets and benchmarks.", "answer": ["Visual Room Rearrangement", "AI2-THOR: An Interactive 3D Environment for Visual AI", "Describe, Explain, Plan and Select: Interactive Planning with Large\n  Language Models Enables Open-World Multi-Task Agents", "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language\n  Models", "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual,\n  Interactive, and Ecological Environments", "DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models", "Open X-Embodiment: Robotic Learning Datasets and RT-X Models", "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations", "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta\n  Reinforcement Learning", "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning", "Mapping Instructions to Actions in 3D Environments with Visual Goal\n  Prediction", "LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic\n  Tabletop Manipulation", "Visually Grounded Task and Motion Planning for Mobile Manipulation", "RoboGPT: an intelligent agent of making embodied long-term decisions for\n  daily instruction tasks", "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks", "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon   Robotics Tasks", "VIMA: General Robot Manipulation with Multimodal Prompts", "RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots", "PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation   Tasks", "LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks\n  in Cluttered Tabletop Environments", "PDDLStream: Integrating Symbolic Planners and Blackbox Samplers via\n  Optimistic Adaptive Planning", "Train Offline, Test Online: A Real Robot Learning Benchmark", "Visually-Grounded Planning without Vision: Language Models Infer\n  Detailed Plans from High-level Instructions", "RH20T-P: A Primitive-Level Robotic Dataset Towards Composable\n  Generalization Agents", "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon\n  Complex Manipulation", "MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task\n  Planning with Open-Source Large Language Model", "RoboCAS: A Benchmark for Robotic Manipulation in Complex Object   Arrangement Scenarios", "RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot", "LLM+P: Empowering Large Language Models with Optimal Planning\n  Proficiency", "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "RePLan: Robotic Replanning with Perception and Language Models", "CALVIN: A Benchmark for Language-Conditioned Policy Learning for\n  Long-Horizon Robot Manipulation Tasks", "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning", "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition", "CAMPs: Learning Context-Specific Abstractions for Efficient Planning in\n  Factored MDPs", "Relevance-driven Decision Making for Safer and More Efficient Human   Robot Collaboration", "Autonomous Planning Based on Spatial Concepts to Tidy Up Home\n  Environments with Service Robots", "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models", "Housekeep: Tidying Virtual Households using Commonsense Reasoning", "RLBench: The Robot Learning Benchmark &amp;amp; Learning Environment", "Task and Motion Planning for Execution in the Real", "robosuite: A Modular Simulation Framework and Benchmark for Robot\n  Learning", "Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback", "Cognitive Mapping and Planning for Visual Navigation", "Robot Task Planning and Situation Handling in Open Worlds", "Habitat 2.0: Training Home Assistants to Rearrange their Habitat", "TidyBot: Personalized Robot Assistance with Large Language Models", "Text2Motion: From Natural Language Instructions to Feasible Plans", "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement\n  Learning for Robust Decision-Making", "Deep Visual Reasoning: Learning to Predict Action Sequences for Task and\n  Motion Planning from an Initial Scene Image", "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and\n  Transfer Learning", "A framework for training and benchmarking algorithms that schedule robot tasks", "FetchBench: A Simulation Benchmark for Robot Fetching", "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models", "Language-Conditioned Robotic Manipulation with Fast and Slow Thinking", "Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents", "Multi-agent Planning using Visual Language Models", "Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments", "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments", "HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation", "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents", "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "HomeRobot: Open-Vocabulary Mobile Manipulation", "DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects", "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"], "answer_arxiv_id": ["2103.16544", "1712.05474", "2302.01560", "2309.10062", "2108.03332", "2404.03275", "2310.08864", "2402.10885", "1910.10897", "2311.17842", "1809.00786", "2310.12020", "2202.10667", "2311.15649", "1912.01734", "2405.01534", "2210.03094", "2406.02523", "2407.00278", "2312.12036", "1802.08705", "2306.00942", "2009.14259", "2403.19622", "2305.12821", "2403.18760", "2407.06951", "2307.00595", "2304.11477", "2303.04137", "2401.04157", "2112.03227", "2307.06135", "2307.14535", "2007.13202", "2409.13998", "2002.03671", "2307.05973", "2205.10712", "1909.12271", "2406.03641", "2009.12293", "2402.08546", "1702.03920", "2210.01287", "2106.14405", "2305.05658", "2303.12153", "1804.07779", "2006.05398", "2010.04296", "2408.16844", "2406.11793", "2310.15127", "2401.04181", "2308.07241", "2408.05478", "2301.04195", "2401.12975", "2403.10506", "2402.08178", "2307.04738", "2306.11565", "2305.05706", "2310.02071"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_46"}
{"question": "How can LLM agents be evaluated and benchmarked for financial tasks? Note that I am referring to agents.", "answer": ["FinBen: A Holistic Financial Benchmark for Large Language Models", "Financial Knowledge Large Language Model", "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents", "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models"], "answer_arxiv_id": ["2402.12659", "2407.00365", "2409.14913", "2308.09975"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_47"}
{"question": "Papers that explore using large language models for mining factors in stock exchange analysis.", "answer": ["Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?", "Automate Strategy Finding with LLM in Quant investment", "LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction", "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models", "Linking microblogging sentiments to stock price movement: An application of GPT-4", "Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment", "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications", "Background-aware Multi-source Fusion Financial Trend Forecasting Mechanism"], "answer_arxiv_id": ["2306.14222", "2409.06289", "2406.10811", "2304.07619", "2308.16771", "2308.00016", "2403.12285", "2407.00904"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_48"}
{"question": "Can you help me find research papers that explore the use of large vision-language models as agents to automatically play PC games?", "answer": ["Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case", "Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study", "Octopus: Embodied Vision-Language Programmer from Environmental Feedback", "Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games", "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models", "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft", "Will GPT-4 Run DOOM?", "GROOT: Learning to Follow Instructions by Watching Gameplay Videos"], "answer_arxiv_id": ["2409.12889", "2403.03186", "2310.08588", "2408.15950", "2311.05997", "2306.00937", "2403.05468", "2310.08235"], "source_meta": {"published_time": "20241001"}, "qid": "RealScholarQuery_49"}
